name: Run All Crawlers Daily

on:
  schedule:
    - cron: '0 0 * * *' # 매일 자정(UTC)에 실행
  workflow_dispatch: # 수동 실행 기능

jobs:
  run-all-crawlers:
    runs-on: ubuntu-latest
    env:
      TZ: Asia/Seoul
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run all crawlers
        env:
          # 모든 크롤러 및 DB 스크립트에 필요한 환경 변수
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
        run: python run_all_crawlers.py

      - name: Cleanup old daily crawler reports
        env:
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          KEEP_DAYS: 14
        run: python scripts/cleanup_daily_crawler_reports.py
